### elasticsearch有哪些调优方法？

1. **设计阶段调优**

* 根据业务增量需求，采取基于日期模板创建索引，通过rolloverAPI滚动索引；
* 使用别名进行索引管理；
* 每天凌晨定时对索引做force_merge操作，以释放空间；
* 采取冷热分离机制，热数据存储到SSD，提高检索效率；冷数据定期进行shrink操作，以缩减存储；
* 采取curator进行索引的生命周期管理；
* 仅针对需要分词的字段，合理的设置分词器；
* Mapping阶段充分结合各个字段的属性，是否需要检索、是否需要存储等。……..

2. **写入调优**

* 写入前副本数设置为0；
* 写入前关闭refresh_interval设置为-1，禁用刷新机制；
* 写入过程中：采取bulk批量写入；
* 写入后恢复副本数和刷新间隔；
* 尽量使用自动生成的id。

3. **查询调优**

* 禁用wildcard；
* 禁用批量terms(成百上千的场景)；
* 充分利用倒排索引机制，能keyword类型尽量keyword；
* 数据量大时候，可以先基于时间敲定索引再检索；
* 设置合理的路由机制。


4. **其他调优**

部署调优，业务调优等。


### elasticsearch的倒排索引是什么？

通俗解释一下就可以。传统的我们的检索是通过文章，逐个遍历找到对应关键词的位置。而倒排索引，是通过分词策略，形成了词和文章的映射关系表，这种词典+映射表即为倒排索引。有了倒排索引，就能实现O(1)时间复杂度的效率检索文章了，极大的提高了检索效率。

学术的解答方式：

倒排索引，相反于一篇文章包含了哪些词，它从词出发，记载了这个词在哪些文档中出现过，由两部分组成——词典和倒排表。

加分项：倒排索引的底层实现是基于：FST(FiniteStateTransducer)数据结构。
lucene从4+版本后开始大量使用的数据结构是FST。FST有两个优点：

1. 空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间；
2. 查询速度快。O(len(str))的查询时间复杂度。

### elasticsearch索引数据多了怎么办，如何调优，部署

面试官：想了解大数据量的运维能力。

解答：索引数据的规划，应在前期做好规划，正所谓“设计先行，编码在后”，这样才能有效的避免突如其来的数据激增导致集群处理能力不足引发的线上客户检索或者其他业务受到影响。如何调优，这里细化一下：

* **动态索引层面**

基于模板+时间+rolloverapi滚动创建索引，举例：设计阶段定义：blog索引的模板格式为：blog_index_时间戳的形式，每天递增数据。

这样做的好处：不至于数据量激增导致单个索引数据量非常大，接近于上线2的32次幂-1，索引存储达到了TB+甚至更大。

一旦单个索引很大，存储等各种风险也随之而来，所以要提前考虑+及早避免。

* **存储层面**

冷热数据分离存储，热数据(比如最近3天或者一周的数据)，其余为冷数据。对于冷数据不会再写入新数据，可以考虑定期force_merge加shrink压缩操作，节省存储空间和检索效率。

* **部署层面**

一旦之前没有规划，这里就属于应急策略。

结合ES自身的支持动态扩展的特点，动态新增机器的方式可以缓解集群压力，注意：如果之前主节点等规划合理，不需要重启集群也能完成动态新增的。


### 详细描述一下Elasticsearch索引文档的过程

面试官：想了解ES的底层原理，不再只关注业务层面了。
解答：
这里的索引文档应该理解为文档写入ES，创建索引的过程。
文档写入包含：单文档写入和批量bulk写入，这里只解释一下：单文档写入流程。

1. 客户写集群某节点写入数据，发送请求。(如果没有指定路由/协调节点，请求的节点扮演路由节点的角色。)

2. 节点1接受到请求后，使用文档_id来确定文档属于分片0。请求会被转到另外的节点，假定节点3。因此分片0的主分片分配到节点3上。

3. 节点3在主分片上执行写操作，如果成功，则将请求并行转发到节点1和节点2的副本分片上，等待结果返回。所有的副本分片都报告成功，节点3将向协调节点(节点1)报告成功，节点1向请求客户端报告写入成功。

如果面试官再问：第二步中的文档获取分片的过程？

回答：借助路由算法获取，路由算法就是根据路由和文档id计算目标的分片id的过程。1shard=hash(_routing)%(num_of_primary_shards)

### 详细描述一下Elasticsearch搜索的过程？


面试官：想了解ES搜索的底层原理，不再只关注业务层面了。

解答：搜索拆解为“query then fetch”两个阶段。

**query阶段的目的：定位到位置，但不取。**

步骤拆解如下：

1. 假设一个索引数据有5主+1副本共10分片，一次请求会命中(主或者副本分片中)的一个。
2. 每个分片在本地进行查询，结果返回到本地有序的优先队列中。3、第2)步骤的结果发送到协调节点，协调节点产生一个全局的排序列表。

**fetch阶段的目的：取数据。**

路由节点获取所有文档，返回给客户端。

### Elasticsearch在部署时，对Linux的设置有哪些优化方法

面试官：想了解对ES集群的运维能力。

解答：

1. 关闭缓存swap;
2. 堆内存设置为：Min(节点内存/2,32GB);
3. 设置最大文件句柄数；
4. 线程池+队列大小根据业务需要做调整；
5. 磁盘存储raid方式——存储有条件使用RAID10，增加单节点性能以及避免单节点存储故障。

### lucence内部结构是什么？

面试官：想了解你的知识面的广度和深度。

解答：
Lucene是有索引和搜索的两个过程，包含索引创建，索引，搜索三个要点。可以基于这个脉络展开一些。

最近面试一些公司，被问到的关于Elasticsearch和搜索引擎相关的问题，以及自己总结的回答。

### Elasticsearch是如何实现Master选举的？

1. Elasticsearch的选主是ZenDiscovery模块负责的，主要包含Ping(节点之间通过这个RPC来发现彼此)和Unicast(单播模块包含一个主机列表以控制哪些节点需要ping通)这两部分；
2. 对所有可以成为master的节点(node.master:true)根据nodeId字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个(第0位)节点，暂且认为它是master节点。
3. 如果对某个节点的投票数达到一定的值(可以成为master节点数n/2+1)并且该节点自己也选举自己，那这个节点就是master。否则重新选举一直到满足上述条件。
4. 补充：master节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data节点可以关闭http功能。


### Elasticsearch中的节点(比如共20个)，其中的10个选了一个master，另外10个选了另一个master，怎么办？

1. 当集群master候选数量不小于3个时，可以通过设置最少投票通过数量	(discovery.zen.minimum_master_nodes)超过所有候选节点一半以上来解决脑裂问题；
2. 当候选数量为两个时，只能修改为唯一的一个master候选，其他作为data节点，避免脑裂问题。

### 客户端在和集群连接时，如何选择特定的节点执行请求的？

TransportClient利用transport模块远程连接一个elasticsearch集群。它并不加入到集群中，只是简单的获得一个或者多个初始化的transport地址，并以轮询的方式与这些地址进行通信。

### 详细描述一下Elasticsearch索引文档的过程。

协调节点默认使用文档ID参与计算(也支持通过routing)，以便为路由提供合适的分片。

shard=hash(document_id)%(num_of_primary_shards)

1. 当分片所在的节点接收到来自协调节点的请求后，会将请求写入到MemoryBuffer，然后定时(默认是每隔1秒)写入到FilesystemCache，这个从MomeryBuffer到FilesystemCache的过程就叫做refresh；
2. 当然在某些情况下，存在MomeryBuffer和FilesystemCache的数据可能会丢失，ES是通过translog的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到translog中，当Filesystemcache中的数据写入到磁盘中时，才会清除掉，这个过程叫做flush；
3. 在flush过程中，内存中的缓冲将被清除，内容被写入一个新段，段的fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的translog将被删除并开始一个新的translog。
4. flush触发的时机是定时触发(默认30分钟)或者translog变得太大(默认为512M)时；

补充：关于Lucene的Segement：

1. Lucene索引是由多个段组成，段本身是一个功能齐全的倒排索引。
2. 段是不可变的，允许Lucene将新的文档增量地添加到索引中，而不用从头重建索引。
3. 对于每一个搜索请求而言，索引中的所有段都会被搜索，并且每个段会消耗CPU的时钟周、文件句柄和内存。这意味着段的数量越多，搜索性能会越低。
4. 为了解决这个问题，Elasticsearch会合并小段到一个较大的段，提交新的合并段到磁盘，并删除那些旧的小段。


### 详细描述一下Elasticsearch更新和删除文档的过程。


1. 删除和更新也都是写操作，但是Elasticsearch中的文档是不可变的，因此不能被删除或者改动以展示其变更；
2. 磁盘上的每个段都有一个相应的.del文件。当删除请求发送后，文档并没有真的被删除，而是在.del文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del文件中被标记为删除的文档将不会被写入新段。
3. 在新的文档被创建时，Elasticsearch会为该文档指定一个版本号，当执行更新时，旧版本的文档在.del文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。

### 详细描述一下Elasticsearch搜索的过程。

1. 搜索被执行成一个两阶段过程，我们称之为QueryThenFetch；
2. 在初始查询阶段时，查询会广播到索引中每一个分片拷贝(主分片或者副本分片)。每个分片在本地执行搜索并构建一个匹配文档的大小为from+size的优先队列。

PS：在搜索的时候是会查询FilesystemCache的，但是有部分数据还在MemoryBuffer，所以搜索是近实时的。
3. 每个分片返回各自优先队列中所有文档的ID和排序值给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。
4. 接下来就是取回阶段，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个GET请求。每个分片加载并丰富文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。
5. 补充：QueryThenFetch的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFSQueryThenFetch增加了一个预查询的处理，询问Term和Documentfrequency，这个评分更准确，但是性能会变差。

### Elasticsearch在部署时，对Linux的设置有哪些优化方法？

1. 64GB内存的机器是非常理想的，但是32GB和16GB机器也是很常见的。少于8GB会适得其反。
2. 如果你要在更快的CPUs和更多的核心之间选择，选择更多的核心更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。
3. 如果你负担得起SSD，它将远远超出任何旋转介质。基于SSD的节点，查询和索引性能都有提升。如果你负担得起，SSD是一个好的选择。
4. 即使数据中心们近在咫尺，也要避免集群跨越多个数据中心。绝对要避免集群跨越大的地理距离。
5. 请确保运行你应用程序的JVM和服务器的JVM是完全一样的。在
E. sticsearch的几个地方，使用Java的本地序列化。
6. 通过设置gateway.recover_after_nodes、gateway.expected_nodes、gateway.recover_after_time可以在集群重启的时候避免过多的分片交换，这可能会让数据恢复从数个小时缩短为几秒钟。
7. Elasticsearch默认被配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。最好使用单播代替组播。
8. 不要随意修改垃圾回收器(CMS)和各个线程池的大小。
9. 把你的内存的(少于)一半给Lucene(但不要超过32GB！)，通过ES_HEAP_SIZE环境变量设置。
10. 内存交换到磁盘对服务器性能来说是致命的。如果内存交换到磁盘上，一个100微秒的操作可能变成10毫秒。再想想那么多10微秒的操作时延累加起来。不难看出swapping对于性能是多么可怕。
11. Lucene使用了大量的文件。同时，Elasticsearch在节点和HTTP客户端之间进行通信也使用了大量的套接字。所有这一切都需要足够的文件描述符。你应该增加你的文件描述符，设置一个很大的值，如64,000。

补充：索引阶段性能提升方法

1. 使用批量请求并调整其大小：每次批量数据5–15MB大是个不错的起始点。
2. 存储：使用SSD
3. 段和合并：Elasticsearch默认值是20MB/s，对机械磁盘应该是个不错的设置。如果你用的是SSD，可以考虑提高到100–200MB/s。如果你在做批量导入，完全不在意搜索，你可以彻底关掉合并限流。另外还可以增加index.translog.flush_threshold_size设置，从默认的512MB到更大一些的值，比如1GB，这可以在一次清空触发的时候在事务日志里积累出更大的段。
4. 如果你的搜索结果不需要近实时的准确度，考虑把每个索引的index.refresh_interval改到30s。
5. 如果你在做大批量导入，考虑通过设置index.number_of_replicas:0关闭副本。


### 对于GC方面，在使用Elasticsearch时要注意什么？

1. SEE：https://elasticsearch.cn/article/32
2. 倒排词典的索引需要常驻内存，无法GC，需要监控datanode上segmentmemory增长趋势。
3. 各类缓存，fieldcache,filtercache,indexingcache,bulkqueue等等，要设置合理的大小，并且要应该根据最坏的情况来看heap是否够用，也就是各类缓存全部占满的时候，还有heap空间可以分配给其他任务吗？避免采用clearcache等“自欺欺人”的方式来释放内存。
4. 避免返回大量结果集的搜索与聚合。确实需要大量拉取数据的场景，可以采用scan&scrollapi来实现。
5. clusterstats驻留内存并无法水平扩展，超大规模集群可以考虑分拆成多个集群通过tribenode连接。
6. 想知道heap够不够，必须结合实际应用场景，并对集群的heap使用情况做持续的监控。

### Elasticsearch对于大数据量(上亿量级)的聚合如何实现？

Elasticsearch提供的首个近似聚合是cardinality度量。它提供一个字段的基数，即该字段的distinct或者unique值的数目。它是基于HLL算法的。HLL会先对我们的输入作哈希运算，然后根据哈希运算的结果中的bits做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用(更精确＝更多内存)；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。

### 在并发情况下，Elasticsearch如果保证读写一致？

1. 可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；
2. 另外对于写操作，一致性级别支持quorum/one/all，默认为quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。
3. 对于读操作，可以设置replication为sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置replication为async时，也可以通过设置搜索请求参数_preference为primary来查询主分片，确保文档是最新版本。

### 如何监控Elasticsearch集群状态？

Marvel让你可以很简单的通过Kibana监控Elasticsearch。你可以实时查看你的集群健康状态和性能，也可以分析过去的集群、索引和节点指标。

### 介绍一下你们的个性化搜索方案？

SEE基于word2vec和Elasticsearch实现个性化搜索


### 是否了解字典树？

Trie的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。它有3个基本性质：
1. 根节点不包含字符，除根节点外每一个节点都只包含一个字符。
2. 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。
3. 每个节点的所有子节点包含的字符都不相同。


### 拼写纠错是如何实现的？

拼写纠错是基于编辑距离来实现；编辑距离是一种标准的方法，它用来表示经过插入、删除和替换操作从一个字符串转换到另外一个字符串的最小操作步数；

对于拼写纠错，我们考虑构造一个度量空间(MetricSpace)，该空间内任何关系满足以下三条基本条件：

* d(x,y)=0--假如x与y的距离为0，则x=y
* d(x,y)=d(y,x)--x到y的距离等同于y到x的距离
* d(x,y)+d(y,z)>=d(x,z)--三角不等式

1. 根据三角不等式，则满足与query距离在n范围内的另一个字符转B，其与A的距离最大为d+n，最小为d-n。
2. BK树的构造就过程如下：每个节点有任意个子节点，每条边有个值表示编辑距离。所有子节点到父节点的边上标注n表示编辑距离恰好为n。比如，我们有棵树父节点是”book”和两个子节点”cake”和”books”，”book”到”books”的边标号1，”book”到”cake”的边上标号4。从字典里构造好树后，无论何时你想插入新单词时，计算该单词与根节点的编辑距离，并且查找数值为d(neweord,root)的边。递归得与各子节点进行比较，直到没有子节点，你就可以创建新的子节点并将新单词保存在那。比如，插入”boo”到刚才上述例子的树中，我们先检查根节点，查找d(“book”,“boo”)=1的边，然后检查标号为1的边的子节点，得到单词”books”。我们再计算距离d(“books”,“boo”)=2，则将新单词插在”books”之后，边标号为2。
3. 查询相似词如下：计算单词与根节点的编辑距离d，然后递归查找每个子节点标号为d-n到d+n(包含)的边。假如被检查的节点与搜索单词的距离d小于n，则返回该节点并继续查询。比如输入cape且最大容忍距离为1，则先计算和根的编辑距离d(“book”,“cape”)=4，然后接着找和根节点之间编辑距离为3到5的，这个就找到了cake这个节点，计算d(“cake”,“cape”)=1，满足条件所以返回cake，然后再找和cake节点编辑距离是0到2的，分别找到cape和cart节点，这样就得到cape这个满足条件的结果。